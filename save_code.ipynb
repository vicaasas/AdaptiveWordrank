{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.scale_factor = 1\n",
    "        \n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        idx = None,\n",
    "        keyword_position=None,\n",
    "    ):\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "        # 12 batch 16 heads 4 beam \n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        # use_layer=[6,8,4]\n",
    "        # decoder_use_layer=[0]\n",
    "        decoder_use_layer=[0,4,9]\n",
    "        if not self.training:\n",
    "            # attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            \n",
    "            if self.is_decoder and is_cross_attention and idx in decoder_use_layer:\n",
    "                sigmoid = nn.Sigmoid()\n",
    "                prob = sigmoid(attn_weights)\n",
    "                m = torch.mean(prob, dim=-1, keepdim=True)\n",
    "                s = torch.std(prob, dim=-1, keepdim=True)\n",
    "                thresholds = m.detach()\n",
    "                # thresholds = m.detach()-s.detach()\n",
    "                # thresholds = m.detach()+s.detach()\n",
    "                m_idx=0\n",
    "                step = int(prob.size(0)/keyword_position.size(0))\n",
    "                for kp in keyword_position:\n",
    "                    flag_idx=torch.where(kp==-100,False,True)\n",
    "                    kp_idx=kp[flag_idx].int()\n",
    "                    prob[m_idx:m_idx+step,:, kp_idx] = 1\n",
    "                    m_idx+=step\n",
    "                # sample = torch.greater(prob, 0.2).type_as(prob)\n",
    "                sample = torch.where(prob > thresholds, True, False).type_as(prob)\n",
    "                attn_weights_exp = torch.exp(attn_weights)\n",
    "                attn_weights_exp = attn_weights_exp * sample\n",
    "                attn_weights_sum = torch.sum(attn_weights_exp, dim=-1, keepdim=True)\n",
    "                attn_probs = attn_weights_exp / torch.clamp(attn_weights_sum, min=1e-4)\n",
    "            else:\n",
    "                attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        else:\n",
    "            # if not self.is_decoder:\n",
    "            #     encoder_use_layer=[11,10,9]\n",
    "            #     if idx in encoder_use_layer:\n",
    "            #         sample_prob = RelaxedBernoulli(0.5, logits=attn_weights)\n",
    "            #         y = sample_prob.rsample()\n",
    "            #         y_hard = torch.greater(y, 0.1).type_as(y)\n",
    "            #         sample = (y_hard - y).detach() + y\n",
    "            #         attn_weights_exp = torch.exp(attn_weights)\n",
    "            #         attn_weights_exp = attn_weights_exp * sample\n",
    "            #         attn_weights_sum = torch.sum(attn_weights_exp, dim=-1, keepdim=True)\n",
    "            #         attn_probs = attn_weights_exp / torch.clamp(attn_weights_sum, min=1e-4)\n",
    "                    \n",
    "            # if self.is_decoder and is_cross_attention and False:\n",
    "            if self.is_decoder and is_cross_attention:\n",
    "                # decoder_use_layer=[0,2,4,6]\n",
    "                if idx in decoder_use_layer:\n",
    "                    # if self.training:\n",
    "                    sample_prob = RelaxedBernoulli(0.5, logits=attn_weights)\n",
    "                    y = sample_prob.rsample()\n",
    "                    at_mask = torch.repeat_interleave(attention_mask,self.num_heads,dim=1).view(bsz * self.num_heads, tgt_len, src_len).detach()\n",
    "                    y_masked = y.detach() + at_mask\n",
    "                    valid_mask = y_masked != torch.finfo(attn_weights.dtype).min\n",
    "\n",
    "                    y_final = torch.where(valid_mask, y_masked, torch.tensor(float('nan')))\n",
    "                    m = torch.nanmean(y_final, dim=-1, keepdim=True)\n",
    "                    v = torch.nanmean((y_final - m) ** 2, dim=-1, keepdim=True)\n",
    "                    s = torch.sqrt(v)\n",
    "                    # m = torch.mean(y, dim=-1, keepdim=True)\n",
    "                    # s = torch.std(y, dim=-1, keepdim=True)\n",
    "                    thresholds = m.detach()+s.detach()\n",
    "                    m_idx=0\n",
    "                    max_val = torch.max(y)\n",
    "\n",
    "                    # 對於 keyword_position，將 y 設置為比 max_val 稍大的值\n",
    "                    keyword_adjustment = max_val + 0.01  # 確保大於 thresholds\n",
    "                    \n",
    "                    for kp in keyword_position:\n",
    "                        flag_idx=torch.where(kp==-100,False,True)\n",
    "                        kp_idx=kp[flag_idx].int()\n",
    "                        if kp_idx.size(0)!=0:\n",
    "                            mask = torch.zeros_like(y[m_idx:m_idx+self.num_heads])\n",
    "                            mask[:,:,kp_idx] = 1\n",
    "                            \n",
    "                            # 使用 StraightThrough 估計器來保留梯度流動\n",
    "                            y_adjusted = y[m_idx:m_idx+self.num_heads] * (1 - mask) + keyword_adjustment * mask\n",
    "                            y[m_idx:m_idx+self.num_heads] = (y_adjusted - y[m_idx:m_idx+self.num_heads]).detach() + y[m_idx:m_idx+self.num_heads]\n",
    "\n",
    "                            # mask = torch.ones_like(y[m_idx:m_idx+self.num_heads])\n",
    "                            # mask[:,:,kp_idx] = 0\n",
    "                            # hard_mask = torch.zeros_like(y[m_idx:m_idx+self.num_heads])\n",
    "                            # hard_mask[:,:, kp_idx] = 1\n",
    "\n",
    "                            # y[m_idx:m_idx+self.num_heads] = (\n",
    "                            #     (hard_mask - y[m_idx:m_idx+self.num_heads]).detach() + y[m_idx:m_idx+self.num_heads])+\\\n",
    "                            #         ((mask - y[m_idx:m_idx+self.num_heads]).detach() + y[m_idx:m_idx+self.num_heads]) * hard_mask + \\\n",
    "                            #             y[m_idx:m_idx+self.num_heads] * (1 - hard_mask)\n",
    "                                        \n",
    "                        m_idx += self.num_heads\n",
    "\n",
    "                    y_hard = torch.where(y > thresholds, True, False).type_as(y)\n",
    "\n",
    "                    sample = (y_hard - y).detach() + y\n",
    "                    sample = sample * self.scale_factor*1/(idx+1) + sample.detach() * (1 - self.scale_factor*1/(idx+1))\n",
    "\n",
    "                    attn_weights_exp = torch.exp(attn_weights)\n",
    "                    attn_weights_exp = attn_weights_exp * sample\n",
    "                    attn_weights_sum = torch.sum(attn_weights_exp, dim=-1, keepdim=True)\n",
    "                    attn_probs = attn_weights_exp / torch.clamp(attn_weights_sum, min=1e-4)\n",
    "                else:\n",
    "                    attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            else:\n",
    "                attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "            \n",
    "        if not self.training:\n",
    "            # attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "            if self.is_decoder and is_cross_attention:\n",
    "                if idx not in decoder_use_layer:\n",
    "                    attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "            \n",
    "            else:\n",
    "                attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        else:\n",
    "            # if not self.is_decoder:\n",
    "            #     if idx not in encoder_use_layer:\n",
    "            #         attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "            # if self.is_decoder and is_cross_attention and False:\n",
    "            if self.is_decoder and is_cross_attention:\n",
    "                if idx not in decoder_use_layer:\n",
    "                    attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "            \n",
    "            else:\n",
    "                attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "from functools import lru_cache\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from typing import Union, Dict, List\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    # TODO(odashi): Avoid programatic download: it requires unnecessary outbound\n",
    "    # connection and won't work in offline systems.\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "class SUMAttribute:\n",
    "    \"\"\"This class calculates several attributes given a sample summary.\n",
    "\n",
    "    These attributes are all refernce free.\n",
    "    * source_len\n",
    "    * hypothesis_len\n",
    "    * density\n",
    "    * coverage\n",
    "    * compression\n",
    "    * repetition\n",
    "    * novelty\n",
    "    * copy_len\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO(odashi): Use dataclass instead.\n",
    "    Match = namedtuple(\"Match\", (\"summary\", \"text\", \"length\"))\n",
    "\n",
    "    def __call__(self, texts: List[str], summaries: List[str]) -> List[dict]:\n",
    "        \"\"\"Calculate attributes of each pair of text and summary.\n",
    "\n",
    "        Args:\n",
    "            texts: a list of source documents.\n",
    "            summaries: a list of generated summaries.\n",
    "\n",
    "        Returns:\n",
    "            A list of dicts with attributes.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for text, summary in zip(texts, summaries):\n",
    "            out.append(self.cal_attributes_each(text, summary))\n",
    "        return out\n",
    "\n",
    "    @lru_cache(maxsize=10)\n",
    "    def cal_attributes_each(self, text: str, summary: str) -> Dict[str, Union[int , float]]:\n",
    "        \"\"\"For a single instance, calculate the attributes of each text/summary pair.\n",
    "\n",
    "        Args:\n",
    "            text: The text.\n",
    "            summary: The summary.\n",
    "\n",
    "        Returns:\n",
    "            Returns the summary.\n",
    "        \"\"\"\n",
    "        # Normalize text\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        tokenized_summary = word_tokenize(summary)\n",
    "        normalized_text = [str(t).lower() for t in tokenized_text]\n",
    "        normalized_summary = [str(t).lower() for t in tokenized_summary]\n",
    "\n",
    "        # Calculate matches\n",
    "        matches = self.overlap(normalized_summary, normalized_text)\n",
    "        summary_len = len(tokenized_summary)\n",
    "\n",
    "        if summary_len == 0:\n",
    "            density, coverage, compression = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "            # Density\n",
    "            density = sum(float(o.length) ** 2 for o in matches) / summary_len\n",
    "            # Coverage\n",
    "            coverage = sum(float(o.length) for o in matches) / summary_len\n",
    "            # Compression\n",
    "            compression = float(len(tokenized_text)) / summary_len\n",
    "\n",
    "        # Repetition\n",
    "        repetition = self.cal_repetition(summary)\n",
    "        # Novelty\n",
    "        novelty = self.cal_novelty(text, summary)\n",
    "\n",
    "        # Copy length\n",
    "        copy_lens = [o.length for o in matches]\n",
    "        if len(copy_lens) == 0:\n",
    "            copy_len = 0.0\n",
    "        else:\n",
    "            copy_len = sum(copy_lens) / len(copy_lens)\n",
    "        return {\n",
    "            \"attr_density\": density,\n",
    "            \"attr_coverage\": coverage,\n",
    "            \"attr_compression\": compression,\n",
    "            \"attr_repetition\": repetition,\n",
    "            \"attr_novelty\": novelty,\n",
    "            \"attr_copy_len\": copy_len,\n",
    "            \"attr_source_len\": len(normalized_text),\n",
    "            \"attr_hypothesis_len\": len(normalized_summary),\n",
    "        }\n",
    "\n",
    "    def _get_ngrams(self, doc, n):\n",
    "        doc = doc.lower()\n",
    "        doc_sents = sent_tokenize(doc)\n",
    "        _ngrams = []\n",
    "        for sent in doc_sents:\n",
    "            sent = word_tokenize(sent)\n",
    "            _ngrams.extend(list(ngrams(sent, n=n)))\n",
    "        return _ngrams\n",
    "\n",
    "    def cal_novelty(self, text: str, summary: str, n: int = 2) -> float:\n",
    "        \"\"\"Returns the novelty score.\n",
    "\n",
    "        Novelty is the proportion of segments in the summaries that havenâ€™t appeared in\n",
    "        source documents. The segments can be instantiated as n-grams.\n",
    "\n",
    "        Args:\n",
    "            text: The text.\n",
    "            summary: The summary.\n",
    "            n: The order of n-grams used in novelty calculation.\n",
    "\n",
    "        Returns:\n",
    "            The ratio of novel n-grams in the summary.\n",
    "        \"\"\"\n",
    "        cnt_all = 0\n",
    "        cnt_nov = 0\n",
    "        _ngrams_text = self._get_ngrams(text, n=n)\n",
    "        _ngrams_summary = self._get_ngrams(summary, n=n)\n",
    "        counter_text: Counter = Counter(_ngrams_text)\n",
    "        counter_summary: Counter = Counter(_ngrams_summary)\n",
    "        for k, v in counter_summary.items():\n",
    "            cnt_all += v\n",
    "            if k not in counter_text:\n",
    "                cnt_nov += v\n",
    "        if cnt_all == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return cnt_nov / cnt_all\n",
    "\n",
    "    def cal_repetition(self, summary: str, n: int = 3) -> float:\n",
    "        \"\"\"Return the ratio of repeated segments in the summary.\n",
    "\n",
    "        Args:\n",
    "            summary: The summary.\n",
    "            n: The length of the n-grams to be used in the calculation.\n",
    "\n",
    "        Returns:\n",
    "            The number of n-grams that are repeated in the summary.\n",
    "        \"\"\"\n",
    "        cnt_all = 0\n",
    "        cnt_rep = 0\n",
    "        _ngrams = self._get_ngrams(summary, n=n)\n",
    "        counter: Counter = Counter(_ngrams)\n",
    "        for k, v in counter.items():\n",
    "            cnt_all += v\n",
    "            if v >= 2:\n",
    "                cnt_rep += v - 1\n",
    "        if cnt_all == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return cnt_rep / cnt_all\n",
    "\n",
    "    def overlap(self, summary: List[str], text: List[str]) -> List[Match]:\n",
    "        \"\"\"Return a list of Match objects between summary and text.\n",
    "\n",
    "        This is a list of named tuples of the form (summary, text, length):\n",
    "            - summary (int): the start index of the match in the summary\n",
    "            - text (int): the start index of the match in the reference\n",
    "            - length (int): the length of the extractive fragment\n",
    "\n",
    "        Args:\n",
    "            summary: the summary\n",
    "            text: the text\n",
    "\n",
    "        Returns:\n",
    "            A list of Match objects indicating matches between the summary and text.\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        summary_start = 0\n",
    "        text_start = 0\n",
    "        while summary_start < len(summary):\n",
    "            best_match = None\n",
    "            best_match_length = 0\n",
    "            while text_start < len(text):\n",
    "                if summary[summary_start] == text[text_start]:\n",
    "                    summary_end = summary_start\n",
    "                    text_end = text_start\n",
    "                    while (\n",
    "                        summary_end < len(summary)\n",
    "                        and text_end < len(text)\n",
    "                        and text[text_end] == summary[summary_end]\n",
    "                    ):\n",
    "                        text_end += 1\n",
    "                        summary_end += 1\n",
    "                    length = summary_end - summary_start\n",
    "                    if length > best_match_length:\n",
    "                        best_match = SUMAttribute.Match(\n",
    "                            summary_start, text_start, length\n",
    "                        )\n",
    "                        best_match_length = length\n",
    "                    text_start = text_end\n",
    "                else:\n",
    "                    text_start += 1\n",
    "            text_start = 0\n",
    "            if best_match:\n",
    "                if best_match_length > 0:\n",
    "                    matches.append(best_match)\n",
    "                summary_start += best_match_length\n",
    "            else:\n",
    "                summary_start += 1\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert_score import score\n",
    "def process(x):\n",
    "    return sent_tokenize(\" \".join(word_tokenize(x.strip())))\n",
    "d,r=[],[]\n",
    "predict = []\n",
    "reference=[]\n",
    "# rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "# rouge1, rouge2, rougeLsum = 0, 0, 0\n",
    "name=\"cache_06-14-11-06-1718334387_transformer_NN_VB_JJ_RB_CD_again_continue_3_model_cur1\"\n",
    "for cnt in range(11490):\n",
    "    with open(\"./result/%s/candidate_ranking/%d.dec\"%(name, cnt), \"r\") as dec:\n",
    "        with open(\"./result/%s/reference_ranking/%d.ref\"%(name, cnt), \"r\") as ref:\n",
    "            x=process(ref.read().replace(\"\\n\",\" \"))\n",
    "            y=process(dec.read().replace(\"\\n\",\" \"))\n",
    "            predict.append(\" \".join(y))\n",
    "            reference.append(\" \".join(x))\n",
    "            # score = rouge_scorer.score(\"\\n\".join(x), \"\\n\".join(y))\n",
    "            # rouge1 += score[\"rouge1\"].fmeasure\n",
    "            # rouge2 += score[\"rouge2\"].fmeasure\n",
    "            # rougeLsum += score[\"rougeLsum\"].fmeasure\n",
    "            # break\n",
    "# rouge1 = rouge1 / (cnt+1)\n",
    "# rouge2 = rouge2 / (cnt+1)\n",
    "# rougeLsum = rougeLsum / (cnt+1)\n",
    "# print(\"ranking rouge1: %.6f, rouge2: %.6f, rougeL: %.6f\"%(rouge1, rouge2, rougeLsum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m11115088/miniconda3/envs/season/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "test = load_from_disk(\"/mnt/nas4/m11115088/WordRank/Dataset/cnndm_gpt_all\")[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11490it [02:00, 95.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0467482810617267, 0.0467482810617267)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import statistics\n",
    "x = SUMAttribute()\n",
    "res_1=[]\n",
    "res_2=[]\n",
    "for d,p in tqdm(zip(test['document'],predict)):\n",
    "    res_1.append(x.cal_novelty(\n",
    "        text=d,\n",
    "        summary=p,\n",
    "        n=1\n",
    "    ))\n",
    "    res_2.append(x.cal_novelty(\n",
    "        text=d,\n",
    "        summary=p,\n",
    "        n=2\n",
    "    ))\n",
    "statistics.mean(res_1),statistics.mean(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/m11115088/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/m11115088/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from BARTScore.bart_score import BARTScorer\n",
    "import statistics\n",
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "meteor = evaluate.load('meteor')\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bart_scorer = BARTScorer(device='cuda:1', checkpoint='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meteor:  {'meteor': 0.3215493205858537}\n",
      "bleu:  {'bleu': 0.09019974870068764, 'precisions': [0.38704816592407415, 0.12930199086982871, 0.05387568620582575, 0.024550382899373457], 'brevity_penalty': 1.0, 'length_ratio': 1.1524181493221255, 'translation_length': 953641, 'reference_length': 827513}\n",
      "bartscore:  -3.6158353603436075\n"
     ]
    }
   ],
   "source": [
    "# bertscore_result = bertscore.compute(predictions=predict, references=reference, lang=\"en\",verbose=True)\n",
    "# print(\"bertscore: \",statistics.mean(bertscore_result['f1']),statistics.mean(bertscore_result['precision']),statistics.mean(bertscore_result['recall']))\n",
    "results = meteor.compute(predictions=predict, references=reference)\n",
    "print(\"meteor: \",results)\n",
    "results = bleu.compute(predictions=predict, references=reference)\n",
    "print(\"bleu: \",results)\n",
    "result = bart_scorer.score(predict, reference, batch_size=8) # generation scores from the first list of texts to the second list of texts.\n",
    "print(\"bartscore: \",statistics.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data_pd = pd.read_json(path_or_buf=\"/mnt/nas4/m11115088/WordRank/CNNDM_Models_generate/LLM_Teached_BART_CNNDM/generated_predictions.json\")\n",
    "# all_data_pd = pd.read_json(path_or_buf=\"cnndm_test_gpt4_turbo_modify.json\", lines=True)\n",
    "test = load_from_disk(\"/mnt/nas4/m11115088/WordRank/Dataset/cnndm_gpt_all\")['test']\n",
    "test_pd = pd.DataFrame(test)\n",
    "test_pd['model_summary'] = all_data_pd['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [01:43<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [00:03<00:00, 47.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15451561.83 seconds, 0.00 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9044668506704485, 0.9033925881481254, 0.9058175513389528)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore_result = bertscore.compute(predictions=test_pd['model_summary'].to_list(), references=test_pd['summary'].to_list(), lang=\"en\",verbose=True)\n",
    "import statistics\n",
    "statistics.mean(bertscore_result['f1']),statistics.mean(bertscore_result['precision']),statistics.mean(bertscore_result['recall'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "season",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
